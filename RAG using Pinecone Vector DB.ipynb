{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "epyWR5LjCgJ1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epyWR5LjCgJ1",
    "outputId": "cb12f42d-5d2f-4301-f79a-a3f756bb053f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c869b00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c869b00",
    "outputId": "829e8aca-777b-457e-b9d0-8eb39f4d460b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\sujit\\anaconda3\\lib\\site-packages (1.48.0)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\sujit\\anaconda3\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from pinecone-client) (2021.10.8)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from pinecone-client) (1.26.9)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sujit\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and API key variables\n",
    "\n",
    "# Install necessary packages\n",
    "!pip install openai pinecone-client\n",
    "\n",
    "# All necessary imports\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "#Additional imports\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1e0c3f1",
   "metadata": {
    "id": "c1e0c3f1"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Variable definitions\n",
    "OPENAI_API_KEY = 'openai_api_key'\n",
    "PINECONE_API_KEY='pinecone_api_key'\n",
    "PINECONE_CLOUD = 'aws'\n",
    "PINECONE_REGION = 'us-east-1'\n",
    "\n",
    "# Initialize OpenAI client with the provided API key\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "MAX_TOKENS = 225\n",
    "PINECONE_INDEX_NAME = 'chat-history-69'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fc1020e",
   "metadata": {
    "id": "3fc1020e"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Define the conversation history\n",
    "history = [\n",
    "\"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
    "\"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
    "\"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
    "\"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
    "\"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
    "\"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
    "\"7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.\",\n",
    "\"8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.\",\n",
    "\"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\",\n",
    "\"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\",\n",
    "\"11: User: Hiking sounds fun! Any specific trails? | Bot: There are great trails in the Rockies and the Appalachian Mountains.\",\n",
    "\"12: User: I'll plan a trip. What about indoor activities? | Bot: Indoor activities like reading, cooking, or playing board games.\",\n",
    "\"13: User: Nice! Any good board games? | Bot: Settlers of Catan and Ticket to Ride are both excellent choices.\",\n",
    "\"14: User: I'll try them out. Any movie recommendations? | Bot: 'Inception' and 'The Matrix' are must-watch movies.\",\n",
    "\"15: User: I love those movies! Any TV shows? | Bot: 'Breaking Bad' and 'Stranger Things' are very popular.\",\n",
    "\"16: User: Great choices! What about podcasts? | Bot: 'How I Built This' and 'The Daily' are very informative.\",\n",
    "\"17: User: Thanks! What are some good travel destinations? | Bot: Paris, Tokyo, and Bali are amazing travel spots.\",\n",
    "\"18: User: I'll add them to my list. Any packing tips? | Bot: Roll your clothes to save space and use packing cubes.\",\n",
    "\"19: User: That's helpful! What about travel insurance? | Bot: Always get travel insurance for safety and peace of mind.\",\n",
    "\"20: User: Thanks for the tips! Any last advice? | Bot: Just enjoy your journey and make the most out of your experiences.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "XJARj_HMPwyt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJARj_HMPwyt",
    "outputId": "015c428a-316a-44e7-d17a-dbe7cf40f6ac"
   },
   "outputs": [],
   "source": [
    "def cluster_conversations(history, n_clusters=3, linkage='average'):\n",
    "    \"\"\"\n",
    "    Cluster conversations using Agglomerative Clustering and append cluster names to each conversation entry.\n",
    "\n",
    "    Args:\n",
    "        history (list of str): List of conversation strings.\n",
    "        n_clusters (int): Number of clusters to find.\n",
    "        linkage (str): Linkage criterion for clustering.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Updated conversation history with cluster labels.\n",
    "    \"\"\"\n",
    "    # Load the SpaCy model\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # Generate embeddings for each history item\n",
    "    history_embeddings = np.array([nlp(entry).vector for entry in history])\n",
    "\n",
    "    # Calculate pairwise cosine similarity between each conversation\n",
    "    similarity_matrix = cosine_similarity(history_embeddings)\n",
    "\n",
    "    # Convert similarity to distance (since clustering algorithms work on distances)\n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "    # Perform Agglomerative Clustering with 'precomputed' metric and average linkage\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        affinity='precomputed',  # Set affinity to precomputed since we're passing a distance matrix\n",
    "        linkage=linkage  # Use the chosen linkage criterion\n",
    "    )\n",
    "    clustering.fit(distance_matrix)\n",
    "\n",
    "    # Assign clusters to each conversation entry\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    # Append cluster names to the history entries\n",
    "    for i, label in enumerate(labels):\n",
    "        history[i] = f\"{history[i]} | Cluster: Cluster {label + 1}\"\n",
    "\n",
    "    return history\n",
    "\n",
    "history = cluster_conversations(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cbc832f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cbc832f",
    "outputId": "737f2e89-4b25-4a0b-f903-92d83656e492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'chat-history-69' stats: {'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 20}},\n",
      " 'total_vector_count': 20}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Function to add embeddings to Pinecone\n",
    "def add_embeddings_to_pinecone(history, index_name='chat-history-69'):\n",
    "    \"\"\"Add embeddings to Pinecone without batch processing\"\"\"\n",
    "\n",
    "    # Create index if it doesn't exist\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=3072,\n",
    "            metric='dotproduct',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=PINECONE_CLOUD,\n",
    "                region=PINECONE_REGION\n",
    "            )\n",
    "        )\n",
    "\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    # Process all messages\n",
    "    vectors = []\n",
    "\n",
    "    for msg in history:\n",
    "        msg_num = int(msg.split(':')[0])\n",
    "\n",
    "        # Extract user message and bot response\n",
    "        parts = msg.split(\" | \")\n",
    "        user_msg = parts[0].split(\": User: \")[1]\n",
    "        bot_msg = parts[1].split(\"Bot: \")[1]\n",
    "        cluster_name = parts[2].split(\": \")[1]\n",
    "\n",
    "        # Split user message by period and get the second part if it exists\n",
    "        user_message_split = user_msg.split(\".\")\n",
    "        if len(user_message_split) > 1:\n",
    "            user_msg = user_message_split[1]\n",
    "\n",
    "        # Create embedding for combined message\n",
    "        combined_text = f\"{cluster_name}{user_msg}\"\n",
    "\n",
    "        # Get embedding directly using OpenAI API\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input= combined_text\n",
    "        )\n",
    "        embedding = response.data[0].embedding\n",
    "\n",
    "        vectors.append({\n",
    "            'id': f'msg_{msg_num}',\n",
    "            'values': embedding,\n",
    "            'metadata': {\n",
    "                'message_num': msg_num,\n",
    "                'user_message': user_msg,\n",
    "                'bot_message': bot_msg,\n",
    "                'full_text': msg\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Upsert all vectors in one call\n",
    "    index.upsert(vectors=vectors)\n",
    "\n",
    "    # Get index stats\n",
    "    index_stats = index.describe_index_stats()\n",
    "    print(f\"Index '{index_name}' stats: {index_stats}\")\n",
    "\n",
    "# Add embeddings to Pinecone without batching\n",
    "add_embeddings_to_pinecone(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "689a86ba",
   "metadata": {
    "id": "689a86ba"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Defining the RAG mechanism\n",
    "def retrieve_relevant_history(query, index_name='chat-history-69'):\n",
    "    \"\"\"Retrieve relevant history based on semantic similarity to query\"\"\"\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    # Get query embedding\n",
    "    query_embedding = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input = query\n",
    "        ).data[0].embedding\n",
    "\n",
    "    # Search with increased top_k and lower threshold\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=3,\n",
    "        include_metadata=True,\n",
    "        include_values=False\n",
    "    )\n",
    "\n",
    "    # Filter and sort results\n",
    "    relevant_messages = []\n",
    "    for match in results.matches:\n",
    "        msg_num = match.metadata['message_num']\n",
    "        relevant_messages.append({\n",
    "            'message_num': msg_num,\n",
    "            'text': match.metadata['bot_message'],\n",
    "            'score': match.score\n",
    "        })\n",
    "\n",
    "    # Sort by message number\n",
    "    #relevant_messages.sort(key=lambda x: x['message_num'])\n",
    "    return relevant_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c24b282",
   "metadata": {
    "id": "3c24b282"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Function to prepare the prompt\n",
    "def prepare_prompt(test_prompt, history, index_name='chat-history-69'):\n",
    "    \"\"\"Prepare prompt with improved context integration\"\"\"\n",
    "    # Retrieve relevant messages\n",
    "    relevant_messages = retrieve_relevant_history(test_prompt, index_name=index_name)\n",
    "\n",
    "    # Format context string\n",
    "    context = \"\\nRelevant history:\\n\"\n",
    "    context_refs = []\n",
    "\n",
    "    for msg in relevant_messages:\n",
    "        context += f\"{msg['text']}\\n\"\n",
    "        context_refs.append(str(msg['message_num']))\n",
    "\n",
    "    # Combine prompt with context\n",
    "    final_prompt = f\"\"\"Your a friendly chatbot and instructed to answer the user question based solely on the context and continue the good flow of conversation.\n",
    "                        {context}\\nCurrent query: {test_prompt}\"\"\"\n",
    "\n",
    "    return final_prompt, context_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f2c53cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f2c53cf",
    "outputId": "10d7051b-8612-4c0d-ec1a-6cc42fe45674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Prompt: Do you think it will help me stay fit?\n",
      "\n",
      "Context Referred: Messages 8.0, 7.0, 10.0\n",
      "\n",
      "Final Response: Yes, incorporating a mix of cardio and strength training exercises, along with activities like hiking, can definitely help you stay fit. It's important to find activities you enjoy so you can maintain a consistent routine.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Function to test the prompt\n",
    "def test_final_prompt():\n",
    "    \"\"\"Test the prompt with improved response handling\"\"\"\n",
    "    final_test_prompt = \"Do you think it will help me stay fit?\"\n",
    "\n",
    "    prepared_prompt, context_refs = prepare_prompt(final_test_prompt, history)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Reference the relevant history when responding. You must answer the user question based mainly on the context provided. If you are not sure of the answer just say I don't know, don't hallucinate.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prepared_prompt}\n",
    "        ],\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    print(f\"Final Test Prompt: {final_test_prompt}\\n\")\n",
    "    print(f\"Context Referred: Messages {', '.join(context_refs)}\\n\")\n",
    "    print(f\"Final Response: {response.choices[0].message.content}\")\n",
    "\n",
    "# Run the test\n",
    "test_final_prompt()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
